{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList = [['my','dog','has','flea',\\\n",
    "               'porblems','help','please'],\n",
    "              ['maybe','not','take','him'\\\n",
    "              'to','dog','park','stupid'],\n",
    "              ['my','dalmation','is','so','cute',\\\n",
    "              'I','love','him'],\n",
    "              ['stop','posting','stupid','worthless','garbage'],\n",
    "              ['mr','licks','ate','my','steak','how'\\\n",
    "              'to','stop','him'],\n",
    "              ['quit','buying','worthless','dog','food','stupid']]\n",
    "classVec = [0,1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet|set(document)  #使用并集构成一个没有重复词汇的集合set\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#要注意到其中的某一词条件概率为0时，其不一定是不出现,因而将其初值设为1\n",
    "def trainNB0(data,label):\n",
    "    da = np.array(data)\n",
    "    la = np.array(label)\n",
    "    uni = list(set(label))\n",
    "    valueList = creatVocabList(data)\n",
    "    P_c = {}\n",
    "    P_c_w = {}\n",
    "    for u in uni:\n",
    "        P_c[u] = len(la[la==u])/len(label) #ndarray才可用[A==B]\n",
    "        data_ci = da[la==u]\n",
    "        value_ci = creatVocabList(data_ci)\n",
    "        P_ci_w = {}      \n",
    "        length = len(valueList)   \n",
    "        for i in valueList:\n",
    "            P_ci_w[i] = 1\n",
    "        for line in data_ci:\n",
    "            length += len(line)\n",
    "        for value in value_ci:\n",
    "            for line_ci in data_ci:\n",
    "                line_ci = np.array(line_ci)\n",
    "                P_ci_w[value] += len(line_ci[line_ci == value])\n",
    "        for key in list(P_ci_w.keys()):    #对于.keys()需要转化为keys才能使用\n",
    "            P_ci_w[key] /= length\n",
    "        P_c_w[u] = P_ci_w\n",
    "    return P_c,P_c_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,material):\n",
    "    P_c = material[0]\n",
    "    P_c_w = material[1]\n",
    "    P = {}\n",
    "    possible = 0\n",
    "    for c in list(P_c.keys()):\n",
    "        P[c] = 0\n",
    "    print(P)\n",
    "    if 'jose' not in list(P_c_w.keys()):\n",
    "        print('is in')\n",
    "    for word in vec2Classify:\n",
    "        print(word)\n",
    "        P[c] += math.log(P_c_w[c][word])\n",
    "    P[c] += math.log(P_c[c])\n",
    "    return max(P,key = P.get)   #找到value最大值对应的键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    regEx = re.compile('\\\\W*')\n",
    "    listOfTokens = regEx.split(bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList=[];classList=[];fullText=[]\n",
    "    for i in range(1,26):\n",
    "        fileName = 'C:/Users/Administrator/Desktop/ham/%d.txt'%i\n",
    "        wordList = textParse(open(fileName).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        fileName = 'C:/Users/Administrator/Desktop/spam/%d.txt'%i\n",
    "        wordList = textParse(open(fileName).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = creatVocabList(docList)\n",
    "    trainingSet = list(range(50));testSet = []\n",
    "    #找出10个随机数作为随机序号\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for index in testSet:\n",
    "        trainMat.append(docList[index])\n",
    "        trainClasses.append(classList[index])\n",
    "    material = trainNB0(trainMat,trainClasses)\n",
    "    error = 0\n",
    "    for index in trainingSet:\n",
    "        for word in docList[index]:\n",
    "            if word not in vocabList:\n",
    "                \n",
    "        if classifyNB(docList[index],material) != classList[index]:\n",
    "            error += 1\n",
    "    print('the error rate is: ',error/len(trainingSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0}\n",
      "is in\n",
      "codeine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: split() requires a non-empty pattern match.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'codeine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-b80a62153a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspamTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-1f9decdf42d6>\u001b[0m in \u001b[0;36mspamTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mclassifyNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaterial\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mclassList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'the error rate is: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-cd70c0162f9d>\u001b[0m in \u001b[0;36mclassifyNB\u001b[1;34m(vec2Classify, material)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvec2Classify\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP_c_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP_c\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#找到value最大值对应的键\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'codeine'"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
